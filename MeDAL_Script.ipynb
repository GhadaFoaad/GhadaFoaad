{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SCRIPT FOR CREATING A REPRODUCIBLE SUBSET AND CROSS-VALIDATION FOLDS\n",
        "# ==============================================================================\n",
        "#\n",
        "# PURPOSE:\n",
        "# This script takes a large dataset (e.g., the full MeDAL corpus) and\n",
        "# creates a smaller, reproducible subset for experimentation. It also generates\n",
        "# stratified 10-fold cross-validation splits to ensure that results are robust\n",
        "# and that future research can use the exact same data partitions for fair\n",
        "# comparison.\n",
        "#\n",
        "# Author: G. M. Farouk\n",
        "# Date: 07/2025\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "\n",
        "\n",
        "# The number of cross-validation folds to create\n",
        "N_SPLITS = 10\n",
        "\n",
        "# A fixed random seed to ensure all random operations are reproducible\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# --- The column name in the CSV that will be CREATED to hold the unique ID for each row.\n",
        "INSTANCE_ID_COLUMN = 'instance_id'\n",
        "\n",
        "# --- The column name for your classification target.\n",
        "# This is used for stratified splitting to maintain class balance in each fold.\n",
        "# UPDATED to match your dataset's \"LABEL\" column.\n",
        "TARGET_COLUMN = 'LABEL'\n"
      ],
      "metadata": {
        "id": "7DoqkgpgZ0bh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Paths to original dataset download\n",
        "!wget -nc -P data/ https://zenodo.org/record/4482922/files/train.csv\n",
        "!wget -nc -P data/ https://zenodo.org/record/4482922/files/valid.csv\n",
        "!wget -nc -P data/ https://zenodo.org/record/4482922/files/test.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvaxACz-Ge1x",
        "outputId": "20a765a3-cf66-45ec-9af7-714daa4ebd57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-28 18:25:24--  https://zenodo.org/record/4482922/files/train.csv\n",
            "Resolving zenodo.org (zenodo.org)... 188.184.103.159, 188.185.79.172, 188.184.98.238, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.184.103.159|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/4482922/files/train.csv [following]\n",
            "--2024-09-28 18:25:24--  https://zenodo.org/records/4482922/files/train.csv\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3541556520 (3.3G) [text/plain]\n",
            "Saving to: ‘data/train.csv’\n",
            "\n",
            "train.csv           100%[===================>]   3.30G  11.1MB/s    in 4m 27s  \n",
            "\n",
            "2024-09-28 18:29:52 (12.6 MB/s) - ‘data/train.csv’ saved [3541556520/3541556520]\n",
            "\n",
            "--2024-09-28 18:29:52--  https://zenodo.org/record/4482922/files/valid.csv\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.79.172, 188.184.98.238, 188.184.103.159, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.79.172|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/4482922/files/valid.csv [following]\n",
            "--2024-09-28 18:29:53--  https://zenodo.org/records/4482922/files/valid.csv\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1180795804 (1.1G) [text/plain]\n",
            "Saving to: ‘data/valid.csv’\n",
            "\n",
            "valid.csv           100%[===================>]   1.10G  17.8MB/s    in 59s     \n",
            "\n",
            "2024-09-28 18:30:52 (19.0 MB/s) - ‘data/valid.csv’ saved [1180795804/1180795804]\n",
            "\n",
            "--2024-09-28 18:30:53--  https://zenodo.org/record/4482922/files/test.csv\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.79.172, 188.184.98.238, 188.184.103.159, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.79.172|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/4482922/files/test.csv [following]\n",
            "--2024-09-28 18:30:53--  https://zenodo.org/records/4482922/files/test.csv\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1180152075 (1.1G) [text/plain]\n",
            "Saving to: ‘data/test.csv’\n",
            "\n",
            "test.csv            100%[===================>]   1.10G  12.7MB/s    in 1m 40s  \n",
            "\n",
            "2024-09-28 18:32:33 (11.3 MB/s) - ‘data/test.csv’ saved [1180152075/1180152075]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"medal\")"
      ],
      "metadata": {
        "id": "iyDE1xGAbDwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to local/drive storage of dataset (updated by user to his dataset downloaded location)\n",
        "DRIVE_PATH = '/content/drive/My Drive/'\n",
        "# Path where the output files will be saved\n",
        "OUTPUT_PATH = '/content/drive/My Drive/data_for_publication/'\n",
        "\n",
        "# Name of your original dataset file\n",
        "INPUT_FILENAME = 'medal_training.csv'\n",
        "\n",
        "# --- 2. SETUP AND DATA LOADING ---\n",
        "\n",
        "print(\"--- Starting Reproducible Subset Creation ---\")\n",
        "# Mount Google Drive to access files\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True) # force_remount can help avoid issues\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    # Exit if drive mounting fails\n",
        "    exit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOANolV4b3n5",
        "outputId": "ae6f362c-c171-419a-df01-5aab20187ef6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Reproducible Subset Creation ---\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the output directory if it doesn't exist\n",
        "print(f\"Creating output directory: {OUTPUT_PATH}\")\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "print(f\"Output directory ensured at: {OUTPUT_PATH}\")\n",
        "\n",
        "# Load the full downloaded dataset\n",
        "full_dataset_path = os.path.join(DRIVE_PATH, INPUT_FILENAME)\n",
        "try:\n",
        "    print(f\"Loading full dataset from: {full_dataset_path}...\")\n",
        "    df_full = pd.read_csv(full_dataset_path)\n",
        "    print(f\"Successfully loaded dataset with {len(df_full)} rows.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: The file was not found at {full_dataset_path}. Please check the path and filename.\")\n",
        "    exit()\n",
        "\n",
        "# --- [CRITICAL UPDATE] ---\n",
        "# Create the instance_id column since it doesn't exist in the original file.\n",
        "# We will use the original row number (the index) as the unique ID.\n",
        "if INSTANCE_ID_COLUMN not in df_full.columns:\n",
        "    print(f\"'{INSTANCE_ID_COLUMN}' column not found. Creating it from the DataFrame's index...\")\n",
        "    df_full[INSTANCE_ID_COLUMN] = df_full.index\n",
        "    print(\"Successfully created instance_id column.\")\n",
        "else:\n",
        "    print(f\"'{INSTANCE_ID_COLUMN}' column already exists.\")\n",
        "\n",
        "# Verify that necessary columns exist AFTER creating the instance_id\n",
        "if INSTANCE_ID_COLUMN not in df_full.columns or TARGET_COLUMN not in df_full.columns:\n",
        "    print(f\"ERROR: The required columns ('{INSTANCE_ID_COLUMN}' or '{TARGET_COLUMN}') were not found in the CSV.\")\n",
        "    print(\"Please check the column names in your file and the script's configuration.\")\n",
        "    exit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXj3bjsjli_Q",
        "outputId": "bc29b9a0-1d47-47ce-8e20-a7cea7c7787d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating output directory: /content/drive/My Drive/data_for_publication/\n",
            "Output directory ensured at: /content/drive/My Drive/data_for_publication/\n",
            "Loading full dataset from: /content/drive/My Drive/medal_training.csv...\n",
            "Successfully loaded dataset with 100000 rows.\n",
            "'instance_id' column not found. Creating it from the DataFrame's index...\n",
            "Successfully created instance_id column.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract ABBREV term from dataset and put in new column#\n",
        "import re\n",
        "def create_abbrev_and_extract_from_text(df):\n",
        "    \"\"\"\n",
        "    Creates an \"ABBREV\" column by extracting the abbreviation\n",
        "    from TEXT using the word in LOCATION index, stopping at a space.\n",
        "    \"\"\"\n",
        "\n",
        "    def extract_abbr(row):\n",
        "        text = row['TEXT']\n",
        "        location = row['LOCATION']\n",
        "\n",
        "        if isinstance(text, str):\n",
        "           try:\n",
        "             word_index = int(location)\n",
        "             words = re.findall(r'\\b\\w+\\b', text) # Tokenize by word\n",
        "             if 0 <= word_index < len(words):\n",
        "                extracted_abbrev = words[word_index] # Get word at index\n",
        "                return extracted_abbrev\n",
        "           except (ValueError, IndexError):\n",
        "               pass # Handle cases where location is invalid\n",
        "        return ''\n",
        "    df_full['ABBREV'] = df_full.apply(extract_abbr, axis=1)\n",
        "    print(\"Added 'ABBREV' column to DataFrame successfully.\")\n",
        "    return df\n",
        "\n",
        "\n",
        "NEW_CSV_FILE_PATH = \"medal_with_abbrev.csv\"\n",
        "\n",
        "\n",
        "# Add new columns\n",
        "df_full = create_abbrev_and_extract_from_text(df_full)\n",
        "\n",
        "# Save the updated dataframe to a new csv file\n",
        "df_full.to_csv(NEW_CSV_FILE_PATH, index=False, encoding='utf-8')\n",
        "print(f\"Saved updated CSV to: {NEW_CSV_FILE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX4BECcLZlRt",
        "outputId": "6ec5a5bb-d33a-4e3e-8aa8-5f328edfc7b2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 'ABBREV' column to DataFrame successfully.\n",
            "Saved updated CSV to: medal_with_abbrev.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract top frequencues Abbreviations#\n",
        "def extract_top_frequent_abbrevs(df, column_name, top_n):\n",
        "    \"\"\"\n",
        "    Extracts the top N most frequent values from a column in a DataFrame and stores them in a new DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        column_name (str): The name of the column to analyze.\n",
        "        top_n (int): The number of top frequent values to extract.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame with the top N most frequent values and their counts.\n",
        "    \"\"\"\n",
        "    # Count the occurrences of each value in the column\n",
        "    value_counts = df[column_name].value_counts()\n",
        "\n",
        "    # Extract the top N most frequent values\n",
        "    top_values = value_counts.nlargest(top_n)\n",
        "\n",
        "    # Convert the result to a DataFrame\n",
        "    top_df = pd.DataFrame({'ABBREV': top_values.index, 'count': top_values.values})\n",
        "    return top_df\n",
        "\n",
        "def filter_dataframe_by_top_abbrevs(df, column_name, top_abbreviations_df):\n",
        "    \"\"\"\n",
        "    Filters a DataFrame to keep rows where the specified column contains any of the top abbreviations.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        column_name (str): The name of the column containing the abbreviations.\n",
        "        top_abbreviations_df (pd.DataFrame): A DataFrame with the top abbreviations.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A filtered DataFrame containing only the rows with top abbreviations.\n",
        "    \"\"\"\n",
        "    top_abbrevs_list = top_abbreviations_df['ABBREV'].tolist()\n",
        "    filtered_df = df[df[column_name].isin(top_abbrevs_list)]\n",
        "    return filtered_df\n",
        "\n",
        "\n",
        "def save_dataframe_to_csv(df, output_file_path):\n",
        "    \"\"\"Saves the DataFrame to a CSV file.\"\"\"\n",
        "    df.to_csv(output_file_path, index=False)\n",
        "    print(f\"DataFrame saved to: {output_file_path}\")"
      ],
      "metadata": {
        "id": "YNa6qcrkbExC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "column_to_analyze = 'ABBREV'\n",
        "top_count = 500\n",
        "output_csv_file = 'MeDAL_Training_Subset.csv' # Specify the path to where you want to save\n",
        "\n",
        "# Get the top abbreviations\n",
        "top_abbreviations_df = extract_top_frequent_abbrevs(df_full, column_to_analyze, top_count)\n",
        "\n",
        "# Filter the dataframe\n",
        "filtered_df = filter_dataframe_by_top_abbrevs(df_full, column_to_analyze, top_abbreviations_df)\n",
        "\n",
        "# Save the filtered DataFrame to CSV\n",
        "save_dataframe_to_csv(filtered_df, output_csv_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC_1zM7RbIcb",
        "outputId": "250f73a1-6e61-4f5b-e9bb-33f2d95938ed"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame saved to: MeDAL_Training_Subset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 3. CREATE THE REPRODUCIBLE SUBSET ---\n",
        "N_SUBSET =filtered_df.shape[0] # Set the desired size of the subset\n",
        "print(f\"\\nCreating a reproducible subset of {N_SUBSET} instances...\")\n",
        "\n",
        "# We use .sample() with a fixed random_state. This guarantees that the\n",
        "# exact same \"random\" subset is chosen every time this script is run.\n",
        "df_subset = df_full.sample(n=N_SUBSET, random_state=RANDOM_SEED)\n",
        "df_subset = df_subset.reset_index(drop=True) # Reset index for clean processing\n",
        "\n",
        "print(f\"Subset created with {len(df_subset)} rows.\")\n",
        "\n",
        "# --- 4. CREATE STRATIFIED K-FOLD SPLITS ---\n",
        "\n",
        "print(f\"\\nGenerating {N_SPLITS}-fold stratified cross-validation splits...\")\n",
        "\n",
        "# Initialize StratifiedKFold. shuffle=True and a fixed random_state ensure\n",
        "# that the data is shuffled the same way before splitting every time.\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
        "\n",
        "# Add a new 'fold' column to our subset DataFrame to store the fold number\n",
        "df_subset['fold'] = -1\n",
        "\n",
        "# Get the data (X) and target (y) for splitting\n",
        "X = df_subset\n",
        "y = df_subset[TARGET_COLUMN]\n",
        "\n",
        "# Loop through the splits and assign each instance to a fold\n",
        "for fold_num, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "    # The 'test_index' contains the indices for the instances in the current fold\n",
        "    df_subset.loc[test_index, 'fold'] = fold_num\n",
        "    print(f\"Assigned {len(test_index)} instances to fold {fold_num}.\")\n",
        "\n",
        "# Verify that all instances were assigned a fold\n",
        "if (df_subset['fold'] == -1).any():\n",
        "    print(\"WARNING: Some rows were not assigned to a fold. Please check the process.\")\n",
        "else:\n",
        "    print(\"All instances successfully assigned to a fold.\")\n",
        "\n",
        "# --- 5. SAVE THE OUTPUT FILES ---\n",
        "\n",
        "print(\"\\n--- Saving output files for publication ---\")\n",
        "\n",
        "# 1. The full subset data, including the new 'fold' column\n",
        "subset_filename = 'MeDAL_subset_with_folds.csv'\n",
        "subset_path = os.path.join(OUTPUT_PATH, subset_filename)\n",
        "df_subset.to_csv(subset_path, index=False)\n",
        "print(f\"1. Saved subset with fold information to: {subset_path}\")\n",
        "\n",
        "# 2. A simple text file with only the unique instance IDs of the subset\n",
        "ids_filename = 'MeDAL_subset_instance_ids.txt'\n",
        "ids_path = os.path.join(OUTPUT_PATH, ids_filename)\n",
        "df_subset[INSTANCE_ID_COLUMN].to_csv(ids_path, index=False, header=False)\n",
        "print(f\"2. Saved list of instance IDs to: {ids_path}\")\n",
        "\n",
        "# 3. A README file explaining the files\n",
        "readme_filename = 'README.txt'\n",
        "readme_path = os.path.join(OUTPUT_PATH, readme_filename)\n",
        "with open(readme_path, 'w') as f:\n",
        "    f.write(\"DATASET FILES FOR REPRODUCIBILITY\\n\")\n",
        "    f.write(\"=====================================\\n\\n\")\n",
        "    f.write(f\"This folder contains the data subset used in the paper: 'Harrensing Transformer Knowledge: A Novel Approach for Biomedical Abbreviation Disambiguation'.\\n\\n\")\n",
        "    f.write(f\"1. {subset_filename}:\\n\")\n",
        "    f.write(f\"   - Contains the full subset from the data with instances used in the study.\\n\")\n",
        "    f.write(\"   - The 'fold' column indicates which of the 10 cross-validation folds each instance belongs to (0-9).\\n\\n\")\n",
        "    f.write(f\"2. {ids_filename}:\\n\")\n",
        "    f.write(\"   - A lightweight text file containing only the unique instance IDs (corresponding to original row numbers) that make up our subset.\\n\")\n",
        "    f.write(\"   - This file can be used to reconstruct the exact subset from the original full dataset.\\n\")\n",
        "\n",
        "print(f\"3. Saved README file to: {readme_path}\")\n",
        "print(\"\\n--- Process complete! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zszYsytNZCwc",
        "outputId": "873e88c7-1b64-4350-c9e6-17a5865d2cb7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating a reproducible subset of 44669 instances...\n",
            "Subset created with 44669 rows.\n",
            "\n",
            "Generating 10-fold stratified cross-validation splits...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigned 4467 instances to fold 0.\n",
            "Assigned 4467 instances to fold 1.\n",
            "Assigned 4467 instances to fold 2.\n",
            "Assigned 4467 instances to fold 3.\n",
            "Assigned 4467 instances to fold 4.\n",
            "Assigned 4467 instances to fold 5.\n",
            "Assigned 4467 instances to fold 6.\n",
            "Assigned 4467 instances to fold 7.\n",
            "Assigned 4467 instances to fold 8.\n",
            "Assigned 4466 instances to fold 9.\n",
            "All instances successfully assigned to a fold.\n",
            "\n",
            "--- Saving output files for publication ---\n",
            "1. Saved subset with fold information to: /content/drive/My Drive/data_for_publication/MeDAL_subset_with_folds.csv\n",
            "2. Saved list of instance IDs to: /content/drive/My Drive/data_for_publication/MeDAL_subset_instance_ids.txt\n",
            "3. Saved README file to: /content/drive/My Drive/data_for_publication/README.txt\n",
            "\n",
            "--- Process complete! ---\n"
          ]
        }
      ]
    }
  ]
}